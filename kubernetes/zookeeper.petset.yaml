## From kubernetes/contrib/pets/zookeeper/zookeeper.yaml
apiVersion: apps/v1alpha1
kind: PetSet
metadata:
  name: zoo
spec:
  serviceName: "zookeeper"
  replicas: 3
  template:
    metadata:
      labels:
        app: onyx-twitter-sample
        component: zk
      annotations:
        pod.alpha.kubernetes.io/initialized: "true"
        pod.alpha.kubernetes.io/init-containers: '[
            {
                "name": "install",
                "image": "onyxplatform/zookeeper:latest",
                "imagePullPolicy": "Always",
                "command": ["/workdir/install.sh"],
                "volumeMounts": [
                    {
                        "name": "confdir",
                        "mountPath": "/opt/zookeeper/conf"
                    }
                ]
            },
            {
                "name": "bootstrap",
                "image": "onyxplatform/zookeeper:latest",
                "command": ["/workdir/peer-finder"],
                "args": ["-on-start=\"/workdir/on-start.sh\"", "-service=zookeeper"],
                "env": [
                  {
                      "name": "POD_NAMESPACE",
                      "valueFrom": {
                          "fieldRef": {
                              "apiVersion": "v1",
                              "fieldPath": "metadata.namespace"
                          }
                      }
                   }
                ],
                "volumeMounts": [
                    {
                        "name": "datadir",
                        "mountPath": "/tmp/zookeeper"
                    },
                    {
                        "name": "confdir",
                        "mountPath": "/opt/zookeeper/conf"
                    }
                ]
            }
        ]'
    spec:
      containers:
#### Zookeeper
      - name: zk
        image: onyxplatform/zookeeper:latest
        ports:
        - containerPort: 2181
          name: client
        - containerPort: 2888
          name: peer
        - containerPort: 3888
          name: leader-election
        - containerPort: 1234
          name: prometheus
        command:
        - /opt/zookeeper/bin/zkServer.sh
        args:
        - start-foreground
        readinessProbe:
          exec:
            command:
            - sh
            - -c
            - "unset JVMFLAGS && /opt/zookeeper/bin/zkCli.sh ls /"
            # Unset JVMFLAGS so we don't bind to 1234 twice
          initialDelaySeconds: 15
          timeoutSeconds: 5
        env:
        - name: JVMFLAGS
          value: "-javaagent:/opt/config/jmx_exporter/jmx_prometheus_javaagent.jar=1234:/opt/config/prometheus/zookeeper.yaml"
        volumeMounts:
        - name: datadir
          mountPath: /tmp/zookeeper
        - name: confdir
          mountPath: /opt/zookeeper/conf
        - name: zk-jmx-config
          mountPath: /opt/config/prometheus
      volumes:
      - name: confdir
        emptyDir: {}
      - name: workdir
        emptyDir: {}
      - name: zk-jmx-config
        configMap:
          name: zk-jmx-config
#### For each pet in the set, create a Volume Claim.
## Using the annotation volume.alpha.kubernetes.io/storage-class
## invokes the alpha dynamic volume allocator. There is one provided
## for Azure/GCE/AWS and a 4th that will just allocate hostDir's
## NOTE: This must be explicitly enabled like any other `alpha` feature.
  volumeClaimTemplates:
  - metadata:
      name: datadir
      labels:
        app: onyx-twitter-sample
        component: zk
      annotations:
        volume.alpha.kubernetes.io/storage-class: anything
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: 5Gi
---
# A headless service to create DNS records
apiVersion: v1
kind: Service
metadata:
  annotations:
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
    prometheus.io/scrape: "true"
    prometheus.io/port: "1234"
  name: zookeeper
  labels:
    app: onyx-twitter-sample
    component: zk
spec:
  ports:
  - port: 2181
    name: client
  - port: 2888
    name: peer
  - port: 3888
    name: leader-election
  - port: 1234
    name: prometheus
  # *.zk.default.svc.cluster.local
  clusterIP: None
  selector:
    component: zk
---
# A configmap for configuring how we relabel the JMX data collected by our
# prometheus sidecar container
apiVersion: v1
kind: ConfigMap
metadata:
  name: zk-jmx-config
  namespace: default
  labels:
    app: onyx-twitter-sample
    component: zk
data:
  zookeeper.yaml: |+
    rules:
      - pattern: "org.apache.ZooKeeperService<name0=ReplicatedServer_id(\\d)><>(\\w+)"
        name: "zookeeper_$2"
      - pattern: "org.apache.ZooKeeperService<name0=ReplicatedServer_id(\\d), name1=replica.(\\d)><>(\\w+)"
        name: "zookeeper_$3"
        labels:
          replicaId: "$2"
      - pattern: "org.apache.ZooKeeperService<name0=ReplicatedServer_id(\\d), name1=replica.(\\d), name2=(\\w+)><>(\\w+)"
        name: "zookeeper_$4"
        labels:
          replicaId: "$2"
          memberType: "$3"
      - pattern: "org.apache.ZooKeeperService<name0=ReplicatedServer_id(\\d), name1=replica.(\\d), name2=(\\w+), name3=(\\w+)><>(\\w+)"
        name: "zookeeper_$4_$5"
        labels:
          replicaId: "$2"
          memberType: "$3"
